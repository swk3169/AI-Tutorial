import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split\

housing = pd.read_csv("2017.csv")

train_set, test_set = train_test_split(housing, test_size=0.1, random_state=42)
print(len(train_set), "train + ", len(test_set), "test")

# 전처리, 비어 있는 것을 채워넣는 처리
# median
m = train_set.median()  # 데이터의 중간 값을 할당

train_set = train_set.fillna(m) # NaN을 특정 값으로 대체 (자체가 바뀌는 것이 아닌 새로운 값 추가)

# data set 전처리
# label
encoder = LabelEncoder()    # 문자열이나 정수로된 라벨 값을 정수로 변환
data_cat = train_set["Country"]
data_cat_encoded = encoder.fit_transform(data_cat).reshape(-1,1)

# # one-hot
# encoder = OneHotEncoder()   # 해당하는 칸의 정보를 1로 표시, 나머지는 0으로 표시
# housing_cat_1hot = encoder.fit(data_cat_encoded.reshape(-1,1))
# housing_cat_1hot.toarray()

train_set = train_set.drop("Country", axis=1)

# scaler
# StandardScaler 정규화, 평균을 제거하고 단위 분산에 맞게 스케일링하여 피쳐를 표준화.
scaler = StandardScaler(copy=True, with_mean=True, with_std=True)
# 추후의 스케일링에 사용될 평균 및 표준 편차를 계산.
scaler.fit(train_set)

train_set = pd.DataFrame(scaler.transform(train_set),
                         columns=["Happiness.Rank", "Happiness.Score", "Whisker.high", "Whisker.low",
                                  "Economy..GDP.per.Capita.", "Family", "Health..Life.Expectancy.", "Freedom", "Generosity",
                                  "Trust..Government.Corruption.", "Dystopia.Residual"])
train_set["Country"] = data_cat_encoded

trainset_input = train_set.drop("Happiness.Score",axis=1)
trainset_output = train_set["Happiness.Score"]

# 모듈
lin_reg = LinearRegression()    # 선형회귀
# lin_reg = DecisionTreeRegressor() # 결정트리
# lin_reg = RandomForestRegressor() # 랜덤 포레스트
lin_reg.fit(trainset_input, trainset_output)

# 테스트 셋 전처리
m = test_set.median()
test_set = test_set.fillna(m)

# label
encoder = LabelEncoder()
data_cat = test_set["Country"]
data_cat_encoded = encoder.fit_transform(data_cat).reshape(-1,1)

test_set = test_set.drop("Country", axis=1)

# scaler
scaler = StandardScaler(copy=True, with_mean=True, with_std=True)
scaler.fit(test_set)

# 변환
test_set = pd.DataFrame(scaler.transform(test_set),
                        columns=["Happiness.Rank", "Happiness.Score", "Whisker.high", "Whisker.low",
                                  "Economy..GDP.per.Capita.", "Family", "Health..Life.Expectancy.", "Freedom", "Generosity",
                                  "Trust..Government.Corruption.", "Dystopia.Residual"])

test_set["Country"] = data_cat_encoded

testset_input = test_set.drop("Happiness.Score",axis=1)

testset_output = test_set["Happiness.Score"]

predicted = lin_reg.predict(testset_input)

lin_mse = mean_squared_error(testset_output, predicted) # 오류
lin_rmse = np.sqrt(lin_mse)

print(lin_rmse)

# # 결과를 디스플레이
# whisker_high_coef = lin_reg.coef_[0]
# whisker_low_coef = lin_reg.coef_[1]
# economy_gdp_coef = lin_reg.coef_[2]
# family_coef = lin_reg.coef_[3]
# health_coef = lin_reg.coef_[4]
# freedom_coef = lin_reg.coef_[5]
# generosity_coef = lin_reg.coef_[6]
# goverment_coef = lin_reg.coef_[7]
# dystopia_coef = lin_reg.coef_[8]
# bias = lin_reg.intercept_
#
# X = testset_input["Whisker.high"] * whisker_high_coef * testset_input["Whisker.low"] * whisker_low_coef\
#     * testset_input["Economy..GDP.per.Capita."] * economy_gdp_coef * testset_input["Family"] * family_coef * testset_input["Health..Life.Expectancy."] * health_coef * testset_input["Freedom"] * freedom_coef \
#     * testset_input["Generosity"] * generosity_coef * testset_input["Trust..Government.Corruption."] * goverment_coef * testset_input["Dystopia.Residual"] * dystopia_coef
# Y = testset_output
#
# plt.xlabel("testinput")
# plt.ylabel("testoutput")
#
# plt.xlim(0.0, 4.0)
# plt.ylim(0.0, 4.0)
# plt.scatter(X,Y,marker=".")
# plt.show()